<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
  <meta charset="utf-8">
  <meta name="description" content="We present ImProvShow; a novel approach to summarizing the multi-stage edit history (or 'provenance') of an image.">
  <meta name="keywords" content="content authenticity, change summarization, image difference captioning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ImProvShow: Multimodal Fusion for Image Provenance Summarization</title>

  <!-- Tailwind CSS CDN -->
  <script src="https://cdn.tailwindcss.com"></script>

  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
    }
  </style>

</head>
<body class="bg-white text-gray-800 leading-normal">

<header class="py-20 text-center bg-gray-50">
  <div class="container mx-auto px-4">
    <div class="max-w-4xl mx-auto">
      <h1 class="text-4xl sm:text-5xl md:text-6xl font-extrabold tracking-tight text-gray-900 leading-tight">
        ImProvShow: Multimodal Fusion for Image Provenance Summarization
      </h1>
      <div class="mt-6 text-lg text-gray-600 space-y-1">
        <div>
          <span class="font-medium">Alexander Black<sup>1</sup>,</span>
          <span class="font-medium">Jing Shi<sup>2</sup>,</span>
          <span class="font-medium">Yifei Fan<sup>2</sup>,</span>
          <span class="font-medium">John Collomosse<sup>1,2</sup></span>
        </div>
        <div class="mt-2 text-sm text-gray-500 space-y-1">
          <p class="font-medium"><sup>1</sup>Centre for the Decentralized Digital Economy (DECaDE)</p>
          <p class="font-medium"><sup>2</sup>Adobe Research</p>
        </div>
      </div>
      
      <!-- Publication Links -->
      <div class="mt-8 flex justify-center flex-wrap gap-4">
        <!-- Paper Link -->
        <a href="#" class="flex items-center gap-2 px-6 py-3 bg-gray-800 text-white font-medium rounded-full shadow-lg hover:bg-gray-700 transition-colors">
          <svg xmlns="http://www.w3.org/2000/svg" class="w-5 h-5" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7.414A2 2 0 0 0 18.586 6L15 2.414A2 2 0 0 0 15 2Z"/><path d="M10 12h4"/><path d="M10 18h4"/><path d="M10 15h4"/><path d="M8 2v4a2 2 0 0 0 2 2h4"/></svg>
          <span>Paper</span>
        </a>
        <!-- Data Link -->
        <a href="https://huggingface.co/datasets/AlexBlck/mets" class="flex items-center gap-2 px-6 py-3 bg-gray-800 text-white font-medium rounded-full shadow-lg hover:bg-gray-700 transition-colors">
          <img src="./static/images/huggingface.svg" alt="Hugging Face logo" class="w-5 h-5 filter invert">
          <span>Data</span>
        </a>
      </div>
    </div>
  </div>
</header>

<!-- Teaser Section -->
<section class="py-12 bg-gray-100 text-center">
  <div class="container mx-auto px-4">
    <div class="max-w-4xl mx-auto">
      <div class="bg-gray-200 rounded-lg overflow-hidden shadow-lg p-4">
        <img src="./static/images/improvshow_teaser.png" alt="Placeholder teaser image for ImProvShow project" class="w-full h-auto rounded-lg">
      </div>
      <h2 class="mt-6 text-xl text-gray-600">
        <strong>ImProvShow</strong> is capable of processing sequences of images, optionally accompanied by coarse edit annotations, to produce a succinct and informative summary of the differences. We train it with METS â€“ a novel dataset of long image editing sequences paired with machine annotations and human-written summaries at multiple steps. The presence of visual and/or text information at any edit stage is optional, as denoted with grey arrows.
      </h2>
    </div>
  </div>
</section>

<!-- Abstract Section -->
<section id="abstract" class="py-16">
  <div class="container mx-auto max-w-4xl px-4">
    <h2 class="text-3xl font-bold text-gray-900 mb-6">Abstract</h2>
    <div class="text-lg text-gray-700 space-y-4 text-justify">
      <p>
        We present ImProvShow; a novel approach to summarizing the multi-stage edit history (or `provenance') of an image. ImProvShow fuses visual and textual cues to succinctly summarize multiple manipulations applied to an image in a sequence; a novel extension of the classical image difference captioning (IDC) problem.
      </p>
      <p>
        ImProvShow takes as input several intermediate thumbnails of the image editing sequence, as well as any coarse human or machine-generated annotations of the individual manipulations at each stage, if available. We demonstrate that the presence of intermediate images and/or auxiliary textual information improves the model's edit captioning performance.
      </p>
      <p>
        To train ImProvShow, we introduce METS (Multiple Edits and Textual Summaries) - a new open dataset of image editing sequences, with textual machine annotations of each editorial step and human edit summarization captions after the 5th, 10th and 15th manipulation.
      </p>
    </div>
  </div>
</section>

<!-- Dataset Section -->
<section id="dataset" class="py-16 bg-gray-50">
  <div class="container mx-auto max-w-4xl px-4">
    <h2 class="text-3xl font-bold text-gray-900 mb-6">Dataset</h2>
    <div class="text-center">
      <div class="bg-gray-200 rounded-lg overflow-hidden shadow-lg p-4">
        <img src="./static/images/dataset_sequence.png" alt="An example of a sequence of manipulations in METS, showing original and manipulated images with superimposed masks and annotations." class="w-full h-auto rounded-lg">
      </div>
      <p class="mt-4 text-sm text-gray-600">
        An example of a sequence of manipulations in METS. The original image is shown in the first column, followed by the manipulated images. The binary masks of the manipulated regions are superimposed on the images. The machine annotations generated during the sequence creation are shown in orange, while the human annotations are shown in blue.
      </p>
    </div>
  </div>
</section>

<!-- Method Section -->
<section id="method" class="py-16">
  <div class="container mx-auto max-w-4xl px-4">
    <h2 class="text-3xl font-bold text-gray-900 mb-6">Method</h2>
    <div class="text-center">
      <div class="bg-gray-200 rounded-lg overflow-hidden shadow-lg p-4">
        <img src="./static/images/method_architecture.png" alt="Architecture of the ImProvShow model." class="w-full h-auto rounded-lg">
      </div>
      <p class="mt-4 text-sm text-gray-600">
        Architecture of the ImProvShow model. The LLaMA-2 language model is conditioned using the multi-modal instruction template, which includes at least two image features and optional auxiliary textual information. All optional content is placed within dashed boxes. The image features extracted from the ViT image encoder are concatenated in groups of 4 and projected to the LLM embedding space.
      </p>
    </div>
  </div>
</section>

<!-- Results Section -->
<section id="results" class="py-16 bg-gray-50">
  <div class="container mx-auto max-w-4xl px-4">
    <h2 class="text-3xl font-bold text-gray-900 mb-6">Results</h2>
    <div class="text-center mb-12">
      <div class="bg-gray-200 rounded-lg overflow-hidden shadow-lg p-4">
        <img src="./static/images/results_table1.png" alt="Table 1. Performance evaluation in the IDC-MI setting." class="w-full h-auto rounded-lg">
      </div>
      <p class="mt-4 text-sm text-gray-600">
        Table 1. Performance evaluation in the IDC-MI setting shows BLEU-4 (<strong>B4</strong>), CIDEr (<strong>C</strong>),
        METEOR (<strong>M</strong>), ROUGE-L (<strong>R</strong>) and LLM as judge medium (<strong>L (M)</strong>) and high (<strong>L (H)</strong>) scores.
        We report the performance of our model and compare it with GPT3.5 and GPT4-V, varying
        the number of input images and the presence of auxiliary textual information.
      </p>
    </div>
    <div class="text-center">
      <div class="bg-gray-200 rounded-lg overflow-hidden shadow-lg p-4">
        <img src="./static/images/results_table2.png" alt="Table 2: Image difference captioning performance evaluation on CLEVR-Change and PSBattles." class="w-full h-auto rounded-lg">
      </div>
      <p class="mt-4 text-sm text-gray-600">
        Table 2: Image difference captioning performance evaluation on CLEVR-Change and PSBattles. We compare our model with the state-of-the-art models and report BLEU-4 (<strong>B4</strong>),
        CIDEr (<strong>C</strong>), METEOR (<strong>M</strong>) and ROUGE-L (<strong>R</strong>) scores.
      </p>
    </div>
  </div>
</section>

<!-- BibTeX Section -->
<section id="paper" class="py-16">
  <div class="container mx-auto max-w-4xl px-4">
    <h2 class="text-3xl font-bold text-gray-900 mb-6">BibTeX</h2>
    <pre class="bg-gray-800 text-white p-6 rounded-lg overflow-x-auto text-sm"><code>@inproceedings{Black2025ImProvShow,
        title={ImProvShow: Multimodal Fusion for Image Provenance Summarization},
        author={Black Alexander and Shi Jing and Fan Yifei and Collomosse John},
        booktitle={British Machine Vision Conference (BMVC)},
        year={2025}
}</code></pre>
  </div>
</section>

<footer class="bg-gray-800 text-gray-300 py-12">
  <div class="container mx-auto px-4 text-center">
    <div class="space-y-4">
      <p class="text-sm">
        This website is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/" class="text-white hover:underline transition-colors">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p class="text-sm">
        The website template is sourced from <a href="https://nerfies.github.io/" class="text-white hover:underline transition-colors">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
